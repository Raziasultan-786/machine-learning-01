{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raziasultan-786/machine-learning-01/blob/main/CICIDS_2017_ML_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I20doCFgig1W"
      },
      "source": [
        "# CMP7239 Applied Machine Learning Assignment\n",
        "## Network Intrusion Detection using CICIDS 2017 Dataset\n",
        "\n",
        "**Student:** [Your Name]  \n",
        "**University:** Birmingham City University  \n",
        "**Course:** CMP7239 Applied Machine Learning  \n",
        "**Domain:** Cybersecurity - Network Intrusion Detection  \n",
        "\n",
        "### Assignment Overview\n",
        "This notebook implements a complete machine learning pipeline for network intrusion detection using the CICIDS 2017 dataset. The analysis includes data preprocessing, exploratory data analysis, implementation of multiple ML algorithms, and comprehensive performance evaluation.\n",
        "\n",
        "### Dataset Information\n",
        "The CICIDS 2017 dataset contains network traffic data with various types of attacks including:\n",
        "- DDoS attacks\n",
        "- Port Scan attacks\n",
        "- Web attacks\n",
        "- Infiltration attacks\n",
        "- Normal traffic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuzEikYuig1a"
      },
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqfwYEpkig1a"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    roc_curve, auc\n",
        ")\n",
        "\n",
        "# Optional: XGBoost (uncomment if available)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "    print(\"XGBoost is available\")\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available, will use Logistic Regression instead\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cxuob7hig1c"
      },
      "source": [
        "## 2. Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-QZQK5big1c"
      },
      "outputs": [],
      "source": [
        "def load_cicids_data():\n",
        "    \"\"\"\n",
        "    Load and combine all CICIDS 2017 dataset files\n",
        "    Returns: Combined DataFrame\n",
        "    \"\"\"\n",
        "    # List of dataset files\n",
        "    dataset_files = [\n",
        "        'Dataset/Monday-WorkingHours.pcap_ISCX.csv',\n",
        "        'Dataset/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
        "        'Dataset/Wednesday-workingHours.pcap_ISCX.csv',\n",
        "        'Dataset/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
        "        'Dataset/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
        "        'Dataset/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
        "        'Dataset/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
        "        'Dataset/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'\n",
        "    ]\n",
        "\n",
        "    dataframes = []\n",
        "\n",
        "    for file in dataset_files:\n",
        "        try:\n",
        "            print(f\"Loading {file}...\")\n",
        "            df = pd.read_csv(file)\n",
        "            print(f\"  Shape: {df.shape}\")\n",
        "            dataframes.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    # Combine all dataframes\n",
        "    if dataframes:\n",
        "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "        print(f\"\\nCombined dataset shape: {combined_df.shape}\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No data loaded successfully\")\n",
        "        return None\n",
        "\n",
        "# Load the dataset\n",
        "print(\"=== LOADING CICIDS 2017 DATASET ===\")\n",
        "df = load_cicids_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4hgyjrzig1d"
      },
      "outputs": [],
      "source": [
        "# Initial data exploration\n",
        "if df is not None:\n",
        "    print(\"=== INITIAL DATA EXPLORATION ===\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"\\nColumn names:\")\n",
        "    for i, col in enumerate(df.columns):\n",
        "        print(f\"{i+1:2d}. {col}\")\n",
        "\n",
        "    print(f\"\\nData types:\")\n",
        "    print(df.dtypes.value_counts())\n",
        "\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"Dataset not loaded properly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgKfn-7cig1d"
      },
      "source": [
        "## 3. Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYYjdAT4ig1e"
      },
      "outputs": [],
      "source": [
        "def clean_dataset(df):\n",
        "    \"\"\"\n",
        "    Clean the CICIDS dataset by handling missing values, duplicates, and data types\n",
        "    \"\"\"\n",
        "    print(\"=== DATA CLEANING ===\")\n",
        "\n",
        "    # Make a copy to avoid modifying original\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Check for missing values\n",
        "    print(\"Missing values per column:\")\n",
        "    missing_values = df_clean.isnull().sum()\n",
        "    missing_percent = (missing_values / len(df_clean)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing_values,\n",
        "        'Percentage': missing_percent\n",
        "    })\n",
        "    print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "    # Handle missing values\n",
        "    if missing_values.sum() > 0:\n",
        "        print(\"\\nHandling missing values...\")\n",
        "        # For numerical columns, fill with median\n",
        "        numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "        for col in numerical_cols:\n",
        "            if df_clean[col].isnull().sum() > 0:\n",
        "                df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
        "\n",
        "        # For categorical columns, fill with mode\n",
        "        categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            if df_clean[col].isnull().sum() > 0:\n",
        "                df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
        "\n",
        "    # Check for duplicates\n",
        "    duplicates = df_clean.duplicated().sum()\n",
        "    print(f\"\\nDuplicate rows: {duplicates}\")\n",
        "    if duplicates > 0:\n",
        "        df_clean = df_clean.drop_duplicates()\n",
        "        print(f\"Removed {duplicates} duplicate rows\")\n",
        "\n",
        "    # Handle infinite values\n",
        "    print(\"\\nChecking for infinite values...\")\n",
        "    inf_cols = []\n",
        "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_cols:\n",
        "        if np.isinf(df_clean[col]).sum() > 0:\n",
        "            inf_cols.append(col)\n",
        "            # Replace infinite values with column max/min\n",
        "            df_clean[col].replace([np.inf, -np.inf], [df_clean[col].max(), df_clean[col].min()], inplace=True)\n",
        "\n",
        "    if inf_cols:\n",
        "        print(f\"Handled infinite values in columns: {inf_cols}\")\n",
        "    else:\n",
        "        print(\"No infinite values found\")\n",
        "\n",
        "    print(f\"\\nCleaned dataset shape: {df_clean.shape}\")\n",
        "    return df_clean\n",
        "\n",
        "# Clean the dataset\n",
        "if df is not None:\n",
        "    df_clean = clean_dataset(df)\n",
        "else:\n",
        "    print(\"Cannot clean dataset - data not loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08XjtlJEig1e"
      },
      "outputs": [],
      "source": [
        "def prepare_features_and_target(df):\n",
        "    \"\"\"\n",
        "    Prepare features and target variable for machine learning\n",
        "    \"\"\"\n",
        "    print(\"=== FEATURE PREPARATION ===\")\n",
        "\n",
        "    # Identify the target column (usually 'Label' in CICIDS dataset)\n",
        "    target_col = None\n",
        "    possible_target_names = ['Label', 'label', 'Label ', ' Label']\n",
        "\n",
        "    for col_name in possible_target_names:\n",
        "        if col_name in df.columns:\n",
        "            target_col = col_name\n",
        "            break\n",
        "\n",
        "    if target_col is None:\n",
        "        # If no standard label column found, use the last column\n",
        "        target_col = df.columns[-1]\n",
        "        print(f\"No standard label column found, using last column: {target_col}\")\n",
        "\n",
        "    print(f\"Target column: {target_col}\")\n",
        "\n",
        "    # Check target distribution\n",
        "    print(f\"\\nTarget distribution:\")\n",
        "    target_counts = df[target_col].value_counts()\n",
        "    print(target_counts)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Handle categorical features\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    print(f\"\\nCategorical features: {categorical_features}\")\n",
        "\n",
        "    # Encode categorical features\n",
        "    label_encoders = {}\n",
        "    for col in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Encode target variable\n",
        "    target_encoder = LabelEncoder()\n",
        "    y_encoded = target_encoder.fit_transform(y.astype(str))\n",
        "\n",
        "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "    print(f\"Target vector shape: {y_encoded.shape}\")\n",
        "    print(f\"Number of classes: {len(target_encoder.classes_)}\")\n",
        "    print(f\"Classes: {target_encoder.classes_}\")\n",
        "\n",
        "    return X, y_encoded, target_encoder, label_encoders\n",
        "\n",
        "# Prepare features and target\n",
        "if 'df_clean' in locals():\n",
        "    X, y, target_encoder, label_encoders = prepare_features_and_target(df_clean)\n",
        "else:\n",
        "    print(\"Cannot prepare features - cleaned data not available\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}