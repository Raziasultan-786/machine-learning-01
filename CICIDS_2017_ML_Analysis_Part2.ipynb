{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raziasultan-786/machine-learning-01/blob/main/CICIDS_2017_ML_Analysis_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwNtJnfGg8Oy"
      },
      "source": [
        "# CMP7239 Applied Machine Learning Assignment - Part 2\n",
        "## Exploratory Data Analysis and Machine Learning Models\n",
        "\n",
        "**Note:** This is a continuation of the main analysis notebook. Run the first notebook (CICIDS_2017_ML_Analysis.ipynb) before running this one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neNiVZBTg8O4"
      },
      "source": [
        "## 4. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oig-yQOEg8O5"
      },
      "outputs": [],
      "source": [
        "def perform_eda(X, y, target_encoder):\n",
        "    \"\"\"\n",
        "    Perform comprehensive exploratory data analysis\n",
        "    \"\"\"\n",
        "    print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n1. SUMMARY STATISTICS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Dataset shape: {X.shape}\")\n",
        "    print(f\"Number of features: {X.shape[1]}\")\n",
        "    print(f\"Number of samples: {X.shape[0]}\")\n",
        "    print(f\"Number of classes: {len(target_encoder.classes_)}\")\n",
        "\n",
        "    # Feature statistics\n",
        "    print(\"\\nFeature Statistics:\")\n",
        "    feature_stats = X.describe()\n",
        "    display(feature_stats)\n",
        "\n",
        "    # Class distribution\n",
        "    print(\"\\n2. CLASS DISTRIBUTION\")\n",
        "    print(\"=\" * 50)\n",
        "    class_counts = pd.Series(y).value_counts().sort_index()\n",
        "    class_names = target_encoder.classes_\n",
        "\n",
        "    for i, count in enumerate(class_counts):\n",
        "        percentage = (count / len(y)) * 100\n",
        "        print(f\"{class_names[i]}: {count:,} samples ({percentage:.2f}%)\")\n",
        "\n",
        "    # Visualizations\n",
        "    print(\"\\n3. VISUALIZATIONS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Class distribution plot\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    class_counts.plot(kind='bar')\n",
        "    plt.title('Class Distribution (Count)')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    class_percentages = (class_counts / len(y)) * 100\n",
        "    plt.pie(class_percentages, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title('Class Distribution (Percentage)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return feature_stats, class_counts\n",
        "\n",
        "# Perform EDA\n",
        "if 'X' in locals() and 'y' in locals():\n",
        "    feature_stats, class_counts = perform_eda(X, y, target_encoder)\n",
        "else:\n",
        "    print(\"Cannot perform EDA - features and target not prepared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzy6pYtIg8O6"
      },
      "outputs": [],
      "source": [
        "def plot_correlation_analysis(X, sample_size=10000):\n",
        "    \"\"\"\n",
        "    Plot correlation heatmap and feature distributions\n",
        "    \"\"\"\n",
        "    print(\"\\n4. CORRELATION ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Sample data if too large for correlation matrix\n",
        "    if len(X) > sample_size:\n",
        "        print(f\"Sampling {sample_size} rows for correlation analysis...\")\n",
        "        X_sample = X.sample(n=sample_size, random_state=42)\n",
        "    else:\n",
        "        X_sample = X\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = X_sample.corr()\n",
        "\n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    # If too many features, show only top correlations\n",
        "    if len(X.columns) > 20:\n",
        "        # Get features with highest variance\n",
        "        feature_variance = X_sample.var().sort_values(ascending=False)\n",
        "        top_features = feature_variance.head(20).index\n",
        "        correlation_subset = correlation_matrix.loc[top_features, top_features]\n",
        "\n",
        "        sns.heatmap(correlation_subset, annot=False, cmap='coolwarm', center=0,\n",
        "                   square=True, linewidths=0.5)\n",
        "        plt.title('Correlation Heatmap (Top 20 Features by Variance)')\n",
        "    else:\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                   square=True, linewidths=0.5, fmt='.2f')\n",
        "        plt.title('Feature Correlation Heatmap')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Find highly correlated features\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
        "                high_corr_pairs.append((\n",
        "                    correlation_matrix.columns[i],\n",
        "                    correlation_matrix.columns[j],\n",
        "                    correlation_matrix.iloc[i, j]\n",
        "                ))\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(f\"\\nHighly correlated feature pairs (|correlation| > 0.8):\")\n",
        "        for feat1, feat2, corr in high_corr_pairs[:10]:  # Show top 10\n",
        "            print(f\"{feat1} - {feat2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(\"\\nNo highly correlated feature pairs found (|correlation| > 0.8)\")\n",
        "\n",
        "    return correlation_matrix\n",
        "\n",
        "# Perform correlation analysis\n",
        "if 'X' in locals():\n",
        "    correlation_matrix = plot_correlation_analysis(X)\n",
        "else:\n",
        "    print(\"Cannot perform correlation analysis - features not prepared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRt8xXOIg8O7"
      },
      "source": [
        "## 5. Feature Scaling and Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhRcAoUHg8O8"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_ml(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Prepare data for machine learning: scaling and train/test split\n",
        "    \"\"\"\n",
        "    print(\"=== DATA PREPARATION FOR MACHINE LEARNING ===\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Test set shape: {X_test.shape}\")\n",
        "    print(f\"Training set class distribution:\")\n",
        "    print(pd.Series(y_train).value_counts().sort_index())\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(\"\\nFeatures scaled using StandardScaler\")\n",
        "    print(f\"Training set mean: {X_train_scaled.mean():.6f}\")\n",
        "    print(f\"Training set std: {X_train_scaled.std():.6f}\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
        "\n",
        "# Prepare data for ML\n",
        "if 'X' in locals() and 'y' in locals():\n",
        "    X_train, X_test, y_train, y_test, scaler = prepare_data_for_ml(X, y)\n",
        "else:\n",
        "    print(\"Cannot prepare data for ML - features and target not available\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}