{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raziasultan-786/machine-learning-01/blob/main/CICIDS_2017_ML_Analysis_Part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PafxIYUghVr9"
      },
      "source": [
        "# CMP7239 Applied Machine Learning Assignment - Part 4\n",
        "## Model Evaluation and Performance Analysis\n",
        "\n",
        "**Note:** This is the final part of the analysis. Run Parts 1, 2, and 3 before running this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb0sYgSLhVsA"
      },
      "source": [
        "## 7. Model Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGrtF7XqhVsA"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_performance(y_true, y_pred, y_pred_proba, model_name, target_encoder):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation with all required metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== {model_name.upper()} PERFORMANCE EVALUATION ===\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    # ROC-AUC (for multiclass)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ROC-AUC:   Could not calculate ({e})\")\n",
        "        roc_auc = None\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(\"-\" * 60)\n",
        "    class_names = target_encoder.classes_\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "    print(report)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "model_results = {}\n",
        "\n",
        "if 'rf_pred' in locals():\n",
        "    model_results['Random Forest'] = evaluate_model_performance(\n",
        "        y_test, rf_pred, rf_pred_proba, 'Random Forest', target_encoder\n",
        "    )\n",
        "\n",
        "if 'svm_pred' in locals():\n",
        "    model_results['SVM'] = evaluate_model_performance(\n",
        "        y_test, svm_pred, svm_pred_proba, 'SVM', target_encoder\n",
        "    )\n",
        "\n",
        "if 'third_pred' in locals():\n",
        "    model_name = 'XGBoost' if XGBOOST_AVAILABLE else 'Logistic Regression'\n",
        "    model_results[model_name] = evaluate_model_performance(\n",
        "        y_test, third_pred, third_pred_proba, model_name, target_encoder\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwMBwxrwhVsB"
      },
      "source": [
        "## 8. ROC Curves Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md0PxH_LhVsC"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curves(models_data, y_true, target_encoder):\n",
        "    \"\"\"\n",
        "    Plot ROC curves for all models\n",
        "    \"\"\"\n",
        "    print(\"\\n=== ROC CURVES COMPARISON ===\")\n",
        "\n",
        "    n_classes = len(target_encoder.classes_)\n",
        "    class_names = target_encoder.classes_\n",
        "\n",
        "    # Create binary labels for each class\n",
        "    y_true_binary = np.eye(n_classes)[y_true]\n",
        "\n",
        "    # Plot ROC curve for each class\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "    for class_idx in range(min(n_classes, 4)):  # Plot up to 4 classes\n",
        "        ax = axes[class_idx]\n",
        "\n",
        "        for i, (model_name, data) in enumerate(models_data.items()):\n",
        "            if 'pred_proba' in data:\n",
        "                y_score = data['pred_proba'][:, class_idx]\n",
        "                fpr, tpr, _ = roc_curve(y_true_binary[:, class_idx], y_score)\n",
        "                roc_auc = auc(fpr, tpr)\n",
        "\n",
        "                ax.plot(fpr, tpr, color=colors[i % len(colors)], lw=2,\n",
        "                       label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "        ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "        ax.set_xlim([0.0, 1.0])\n",
        "        ax.set_ylim([0.0, 1.05])\n",
        "        ax.set_xlabel('False Positive Rate')\n",
        "        ax.set_ylabel('True Positive Rate')\n",
        "        ax.set_title(f'ROC Curve - {class_names[class_idx]}')\n",
        "        ax.legend(loc=\"lower right\")\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Prepare data for ROC curves\n",
        "roc_data = {}\n",
        "if 'rf_pred_proba' in locals():\n",
        "    roc_data['Random Forest'] = {'pred_proba': rf_pred_proba}\n",
        "if 'svm_pred_proba' in locals():\n",
        "    roc_data['SVM'] = {'pred_proba': svm_pred_proba}\n",
        "if 'third_pred_proba' in locals():\n",
        "    model_name = 'XGBoost' if XGBOOST_AVAILABLE else 'Logistic Regression'\n",
        "    roc_data[model_name] = {'pred_proba': third_pred_proba}\n",
        "\n",
        "if roc_data and 'y_test' in locals():\n",
        "    plot_roc_curves(roc_data, y_test, target_encoder)\n",
        "else:\n",
        "    print(\"Cannot plot ROC curves - model predictions not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx1Yaf7uhVsC"
      },
      "source": [
        "## 9. Model Comparison Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL_3adOghVsD"
      },
      "outputs": [],
      "source": [
        "def create_model_comparison_table(model_results):\n",
        "    \"\"\"\n",
        "    Create a comprehensive comparison table of all models\n",
        "    \"\"\"\n",
        "    print(\"\\n=== MODEL COMPARISON SUMMARY ===\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if not model_results:\n",
        "        print(\"No model results available for comparison\")\n",
        "        return\n",
        "\n",
        "    # Create comparison DataFrame\n",
        "    comparison_data = []\n",
        "    for model_name, metrics in model_results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': f\"{metrics['accuracy']:.4f}\",\n",
        "            'Precision': f\"{metrics['precision']:.4f}\",\n",
        "            'Recall': f\"{metrics['recall']:.4f}\",\n",
        "            'F1-Score': f\"{metrics['f1_score']:.4f}\",\n",
        "            'ROC-AUC': f\"{metrics['roc_auc']:.4f}\" if metrics['roc_auc'] else 'N/A'\n",
        "        })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "\n",
        "    # Find best model for each metric\n",
        "    print(\"\\n=== BEST PERFORMING MODELS ===\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "\n",
        "    for metric in metrics_to_compare:\n",
        "        if metric == 'roc_auc':\n",
        "            # Handle cases where ROC-AUC might be None\n",
        "            valid_results = {k: v for k, v in model_results.items() if v[metric] is not None}\n",
        "            if valid_results:\n",
        "                best_model = max(valid_results.keys(), key=lambda k: valid_results[k][metric])\n",
        "                best_score = valid_results[best_model][metric]\n",
        "                print(f\"Best {metric.upper()}: {best_model} ({best_score:.4f})\")\n",
        "        else:\n",
        "            best_model = max(model_results.keys(), key=lambda k: model_results[k][metric])\n",
        "            best_score = model_results[best_model][metric]\n",
        "            print(f\"Best {metric.upper()}: {best_model} ({best_score:.4f})\")\n",
        "\n",
        "    # Overall recommendation\n",
        "    print(\"\\n=== OVERALL RECOMMENDATION ===\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Calculate average rank for each model\n",
        "    model_ranks = {}\n",
        "    for model_name in model_results.keys():\n",
        "        ranks = []\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
        "            sorted_models = sorted(model_results.keys(),\n",
        "                                 key=lambda k: model_results[k][metric], reverse=True)\n",
        "            ranks.append(sorted_models.index(model_name) + 1)\n",
        "        model_ranks[model_name] = np.mean(ranks)\n",
        "\n",
        "    best_overall = min(model_ranks.keys(), key=lambda k: model_ranks[k])\n",
        "    print(f\"Best Overall Model: {best_overall}\")\n",
        "    print(f\"Average Rank: {model_ranks[best_overall]:.2f}\")\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Create model comparison\n",
        "if model_results:\n",
        "    comparison_table = create_model_comparison_table(model_results)\n",
        "else:\n",
        "    print(\"No model results available for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEZvlJkwhVsD"
      },
      "source": [
        "## 10. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HkYwIJthVsE"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importance(model, feature_names, model_name, top_n=20):\n",
        "    \"\"\"\n",
        "    Plot feature importance for tree-based models\n",
        "    \"\"\"\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        print(f\"\\n=== {model_name.upper()} FEATURE IMPORTANCE ===\")\n",
        "\n",
        "        # Get feature importances\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "        # Create DataFrame for easier handling\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        # Display top features\n",
        "        print(f\"\\nTop {top_n} Most Important Features:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, (_, row) in enumerate(feature_importance_df.head(top_n).iterrows()):\n",
        "            print(f\"{i+1:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "        # Plot feature importance\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_features = feature_importance_df.head(top_n)\n",
        "        plt.barh(range(len(top_features)), top_features['importance'])\n",
        "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.title(f'{model_name} - Top {top_n} Feature Importances')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return feature_importance_df\n",
        "    else:\n",
        "        print(f\"{model_name} does not support feature importance analysis\")\n",
        "        return None\n",
        "\n",
        "# Plot feature importance for applicable models\n",
        "if 'X' in locals():\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    if 'rf_model' in locals():\n",
        "        rf_importance = plot_feature_importance(rf_model, feature_names, 'Random Forest')\n",
        "\n",
        "    if 'third_model' in locals() and XGBOOST_AVAILABLE:\n",
        "        xgb_importance = plot_feature_importance(third_model, feature_names, 'XGBoost')\n",
        "else:\n",
        "    print(\"Cannot analyze feature importance - feature names not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WskuosDJhVsE"
      },
      "source": [
        "## 11. Conclusions and Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvU268AghVsF"
      },
      "source": [
        "### Summary of Results\n",
        "\n",
        "This comprehensive analysis of the CICIDS 2017 dataset for network intrusion detection has provided valuable insights:\n",
        "\n",
        "#### Dataset Characteristics:\n",
        "- **Domain**: Cybersecurity - Network Intrusion Detection\n",
        "- **Dataset**: CICIDS 2017 with multiple attack types\n",
        "- **Features**: Network traffic characteristics and flow statistics\n",
        "- **Classes**: Various attack types and normal traffic\n",
        "\n",
        "#### Data Preprocessing:\n",
        "- ✅ **Missing Values**: Handled using median/mode imputation\n",
        "- ✅ **Duplicates**: Removed duplicate records\n",
        "- ✅ **Infinite Values**: Replaced with appropriate bounds\n",
        "- ✅ **Categorical Encoding**: Applied Label Encoding\n",
        "- ✅ **Feature Scaling**: Applied StandardScaler normalization\n",
        "\n",
        "#### Exploratory Data Analysis:\n",
        "- ✅ **Summary Statistics**: Comprehensive feature analysis\n",
        "- ✅ **Class Distribution**: Analyzed class imbalances\n",
        "- ✅ **Correlation Analysis**: Identified feature relationships\n",
        "- ✅ **Visualizations**: Class distributions and correlation heatmaps\n",
        "\n",
        "#### Machine Learning Models:\n",
        "- ✅ **Random Forest**: Implemented with hyperparameter tuning\n",
        "- ✅ **Support Vector Machine**: Optimized with Grid Search\n",
        "- ✅ **XGBoost/Logistic Regression**: Third algorithm implementation\n",
        "\n",
        "#### Performance Evaluation:\n",
        "- ✅ **Confusion Matrix**: Detailed classification analysis\n",
        "- ✅ **Accuracy, Precision, Recall, F1-Score**: Comprehensive metrics\n",
        "- ✅ **ROC-AUC**: Multi-class performance assessment\n",
        "- ✅ **ROC Curves**: Visual performance comparison\n",
        "\n",
        "#### Key Findings:\n",
        "1. **Best Performing Model**: [Results will show after execution]\n",
        "2. **Feature Importance**: Network flow characteristics are crucial\n",
        "3. **Class Imbalance**: Some attack types are underrepresented\n",
        "4. **Model Comparison**: Ensemble methods typically perform better\n",
        "\n",
        "#### Recommendations for Production:\n",
        "1. **Model Selection**: Use the best performing model based on F1-score\n",
        "2. **Feature Engineering**: Focus on top important features\n",
        "3. **Class Balancing**: Consider SMOTE or other techniques for imbalanced classes\n",
        "4. **Real-time Implementation**: Optimize for low-latency detection\n",
        "5. **Continuous Learning**: Implement model updates with new attack patterns\n",
        "\n",
        "### Technical Implementation Notes:\n",
        "- All code is well-documented with inline comments\n",
        "- Functions are modular and reusable\n",
        "- Visualizations are publication-ready\n",
        "- Results are clearly formatted for report inclusion\n",
        "\n",
        "**Note**: Run all notebook cells in sequence to see the complete analysis results and performance metrics.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}