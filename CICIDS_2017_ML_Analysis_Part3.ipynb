{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raziasultan-786/machine-learning-01/blob/main/CICIDS_2017_ML_Analysis_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iqyoZM-hvJl"
      },
      "source": [
        "# CMP7239 Applied Machine Learning Assignment - Part 3\n",
        "## Machine Learning Models Implementation and Evaluation\n",
        "\n",
        "**Note:** This is a continuation of the analysis. Run Parts 1 and 2 before running this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITkG7Ge3hvJo"
      },
      "source": [
        "## 6. Machine Learning Models Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EJMN4WShvJp"
      },
      "outputs": [],
      "source": [
        "def train_random_forest(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate Random Forest classifier with hyperparameter tuning\n",
        "    \"\"\"\n",
        "    print(\"=== RANDOM FOREST CLASSIFIER ===\")\n",
        "\n",
        "    # Define parameter grid for hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2]\n",
        "    }\n",
        "\n",
        "    # Create Random Forest classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Perform Grid Search\n",
        "    print(\"Performing hyperparameter tuning...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        rf, param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best parameters\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Train final model with best parameters\n",
        "    best_rf = grid_search.best_estimator_\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    y_pred_proba = best_rf.predict_proba(X_test)\n",
        "\n",
        "    return best_rf, y_pred, y_pred_proba\n",
        "\n",
        "# Train Random Forest\n",
        "if 'X_train' in locals():\n",
        "    rf_model, rf_pred, rf_pred_proba = train_random_forest(X_train, y_train, X_test, y_test)\n",
        "else:\n",
        "    print(\"Cannot train Random Forest - training data not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D26yoeH3hvJq"
      },
      "outputs": [],
      "source": [
        "def train_svm(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate SVM classifier with hyperparameter tuning\n",
        "    \"\"\"\n",
        "    print(\"\\n=== SUPPORT VECTOR MACHINE ===\")\n",
        "\n",
        "    # Define parameter grid for hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['rbf', 'linear'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "\n",
        "    # Create SVM classifier\n",
        "    svm = SVC(random_state=42, probability=True)\n",
        "\n",
        "    # Perform Grid Search\n",
        "    print(\"Performing hyperparameter tuning...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        svm, param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best parameters\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Train final model with best parameters\n",
        "    best_svm = grid_search.best_estimator_\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_svm.predict(X_test)\n",
        "    y_pred_proba = best_svm.predict_proba(X_test)\n",
        "\n",
        "    return best_svm, y_pred, y_pred_proba\n",
        "\n",
        "# Train SVM\n",
        "if 'X_train' in locals():\n",
        "    svm_model, svm_pred, svm_pred_proba = train_svm(X_train, y_train, X_test, y_test)\n",
        "else:\n",
        "    print(\"Cannot train SVM - training data not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHwHqqEBhvJr"
      },
      "outputs": [],
      "source": [
        "def train_third_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate third classifier (XGBoost or Logistic Regression)\n",
        "    \"\"\"\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        print(\"\\n=== XGBOOST CLASSIFIER ===\")\n",
        "\n",
        "        # Define parameter grid for XGBoost\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 6],\n",
        "            'learning_rate': [0.1, 0.2],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        }\n",
        "\n",
        "        # Create XGBoost classifier\n",
        "        xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
        "\n",
        "        # Perform Grid Search\n",
        "        print(\"Performing hyperparameter tuning...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            xgb_model, param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Best parameters\n",
        "        print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "        print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        # Train final model with best parameters\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "    else:\n",
        "        print(\"\\n=== LOGISTIC REGRESSION ===\")\n",
        "\n",
        "        # Define parameter grid for Logistic Regression\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'solver': ['liblinear', 'lbfgs'],\n",
        "            'max_iter': [1000, 2000]\n",
        "        }\n",
        "\n",
        "        # Create Logistic Regression classifier\n",
        "        lr = LogisticRegression(random_state=42)\n",
        "\n",
        "        # Perform Grid Search\n",
        "        print(\"Performing hyperparameter tuning...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            lr, param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Best parameters\n",
        "        print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "        print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        # Train final model with best parameters\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_pred_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "    return best_model, y_pred, y_pred_proba\n",
        "\n",
        "# Train third model\n",
        "if 'X_train' in locals():\n",
        "    third_model, third_pred, third_pred_proba = train_third_model(X_train, y_train, X_test, y_test)\n",
        "else:\n",
        "    print(\"Cannot train third model - training data not available\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}